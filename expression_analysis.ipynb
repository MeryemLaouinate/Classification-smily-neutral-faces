{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdCT+2mdlZHM4LF/Dregwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeryemLaouinate/Classification-smily-neutral-faces/blob/main/expression_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30QKw8wNEaVg",
        "outputId": "c3e9ae2a-79b9-49c4-dbac-bf4780568e09"
      },
      "source": [
        "\r\n",
        "import cv2\r\n",
        "import glob,os\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "print (\"tensorflow version\",tf.__version__)\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruNKqE0aF0xr"
      },
      "source": [
        "# reading images from the data folder\r\n",
        "# label, smile = 1, neutral = 0\r\n",
        "(img_width, img_height) = (45, 25)\r\n",
        "def load_images_from_folder(folder):\r\n",
        "    (images, lables, names, id) = ([], [], {}, 0)\r\n",
        "    for (subdirs, dirs, files) in os.walk(folder):\r\n",
        "        print (subdirs, dirs)\r\n",
        "        for subdir in dirs:\r\n",
        "            names[id] = subdir\r\n",
        "            subjectpath = os.path.join(folder, subdir)\r\n",
        "            for filename in os.listdir(subjectpath):\r\n",
        "                path = subjectpath + '/' + filename\r\n",
        "                lable = id\r\n",
        "                img = cv2.imread(path, 0) \r\n",
        "                img = cv2.resize(img,(img_width, img_height)) \r\n",
        "                images.append(img)\r\n",
        "                lables.append(int(lable))\r\n",
        "            id += 1\r\n",
        "        print(names)\r\n",
        "        print(lables)\r\n",
        "        return images, lables, names"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD53EHlmF6xI",
        "outputId": "1c0e2898-7228-4707-e347-0e0230f09267"
      },
      "source": [
        "X,Y,classes = load_images_from_folder(\"/content/sample_data/Data\")\r\n",
        "\r\n",
        "(X,Y) = [np.array(lis) for lis in [X, Y]]\r\n",
        "Y = pd.get_dummies(Y) #converting labels to one-hot, Used Pandas for it."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/sample_data/Data ['.ipynb_checkpoints', 'neutral', 'smile']\n",
            "{0: '.ipynb_checkpoints', 1: 'neutral', 2: 'smile'}\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgXXMKBpG05r",
        "outputId": "ce9adfbb-32a2-47ae-d397-2fc275e0d3d3"
      },
      "source": [
        "print (\"Height of the image:  \" + str(X.shape[1]))\r\n",
        "print (\"Width of the image:   \"  + str(X.shape[2]))\r\n",
        "print (\"Total number of data: \"+ str(len(X)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Height of the image:  25\n",
            "Width of the image:   45\n",
            "Total number of data: 107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dL4SZCFH4tr"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X, Y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRbTLybeH6jG",
        "outputId": "7d0448e2-d105-40e3-9a8e-dc27d508e437"
      },
      "source": [
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X, Y, test_size=0.33, random_state=42)\r\n",
        "print (\"lenght of train data: \"+str(len(X_train))) \r\n",
        "print (\"lenght of test data:  \"+str(len(X_test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lenght of train data: 71\n",
            "lenght of test data:  36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbuOcJdIIB2d",
        "outputId": "d38f1308-9809-4ae6-a9e0-98eabfd2811c"
      },
      "source": [
        "train_set_x_flatten = X_train.reshape(X_train.shape[0],-1)\r\n",
        "test_set_x_flatten = X_test.reshape(X_test.shape[0],-1)\r\n",
        "print (\"train_set_x_flatten: \"+str(train_set_x_flatten.shape))\r\n",
        "print (\"test_set_x_flatten : \"+str(test_set_x_flatten.shape))\r\n",
        "\r\n",
        "print (train_set_x_flatten)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_set_x_flatten: (71, 1125)\n",
            "test_set_x_flatten : (36, 1125)\n",
            "[[ 99 110 109 ... 134 131 124]\n",
            " [131 134 136 ... 123 122 117]\n",
            " [163 165 166 ... 161 160 155]\n",
            " ...\n",
            " [124 123 124 ... 192 200 204]\n",
            " [173 181 188 ... 166 163 159]\n",
            " [158 145 156 ... 150 140 142]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqor3x7cIFuS",
        "outputId": "f5509845-e8df-4716-94fc-46a1c0760b8a"
      },
      "source": [
        "#Normalizing data, conveting all pixel value in range between 0-1\r\n",
        "train_set_x = train_set_x_flatten/255.\r\n",
        "test_set_x = test_set_x_flatten/255.\r\n",
        "print (\"length of train set: \" + str(len(train_set_x)))\r\n",
        "print (\"length of test set: \" + str(len(test_set_x)))\r\n",
        "print (\"shape of train set: \"+ str(train_set_x.shape))\r\n",
        "print (\"shape of test set: \"+ str(test_set_x.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train set: 71\n",
            "length of test set: 36\n",
            "shape of train set: (71, 1125)\n",
            "shape of test set: (36, 1125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwAcm3_IKMp",
        "outputId": "fd039efc-4223-4b24-cbde-0a0be131bdd1"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "\r\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnA3nxTlIQqx"
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, 1125])\r\n",
        "W = tf.Variable(tf.zeros([1125, 2]), dtype=tf.float32, name=\"w1\")\r\n",
        "b = tf.Variable(tf.zeros([2]),  dtype=tf.float32, name=\"bias\")\r\n",
        "y = tf.nn.softmax(tf.matmul(x, W) + b, name=\"first_operation\")\r\n",
        "y_ = tf.placeholder(tf.float32, [None, 2], name = \"final_output\")\r\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DUSUAR8I5nr",
        "outputId": "9401c625-2724-4c55-a66e-7b3492bce690"
      },
      "source": [
        "learning_rate = 0.1\r\n",
        "print (\"training with learning rate: \" + str(learning_rate))\r\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\r\n",
        "\r\n",
        "saver = tf.train.Saver()\r\n",
        "with tf.Session() as sess:\r\n",
        "    sess.run(tf.global_variables_initializer())\r\n",
        "    for step in range(15000):\r\n",
        "        trainStep,loss = sess.run([train_step, cross_entropy], feed_dict={x: train_set_x, y_: y_train})\r\n",
        "        if step%1000==0:\r\n",
        "            print (\"losses after per 1000 iteration: \",loss)\r\n",
        "\r\n",
        "    save_path = saver.save(sess, \"./model/emotion_model\")\r\n",
        "\r\n",
        "    print(\"Model saved in file: %s\" % save_path)\r\n",
        "\r\n",
        "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n",
        "    print(\"accuracy with learning rate: \" ,str(learning_rate), sess.run(accuracy, feed_dict={x:test_set_x , y_: y_test}))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training with learning rate: 0.1\n",
            "losses after per 1000 iteration:  0.6931472\n",
            "losses after per 1000 iteration:  0.011277198\n",
            "losses after per 1000 iteration:  0.0065985005\n",
            "losses after per 1000 iteration:  0.004720176\n",
            "losses after per 1000 iteration:  0.0036882658\n",
            "losses after per 1000 iteration:  0.0030317826\n",
            "losses after per 1000 iteration:  0.0025760354\n",
            "losses after per 1000 iteration:  0.0022406802\n",
            "losses after per 1000 iteration:  0.0019833003\n",
            "losses after per 1000 iteration:  0.0017794182\n",
            "losses after per 1000 iteration:  0.0016138569\n",
            "losses after per 1000 iteration:  0.001476693\n",
            "losses after per 1000 iteration:  0.001361178\n",
            "losses after per 1000 iteration:  0.0012625192\n",
            "losses after per 1000 iteration:  0.0011773005\n",
            "Model saved in file: ./model/emotion_model\n",
            "accuracy with learning rate:  0.1 0.9722222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "723UfUG0J8VI"
      },
      "source": [
        "init_op = tf.global_variables_initializer() "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx_DmPyhJBxp",
        "outputId": "7f53e59c-ac59-47c7-c928-8788e48cc753"
      },
      "source": [
        "\r\n",
        "saver = tf.train.Saver()\r\n",
        "with tf.Session() as sess:\r\n",
        "    #saver.restore(sess, \"/content/model/emotion_model\")\r\n",
        "    print(\"w1:\", sess.run(init_op))\r\n",
        "    #print(\"bias:\", sess.run(b))\r\n",
        "    img = cv2.imread(\"/content/sample_data/test/test1.jpg\", 0) \r\n",
        "    img = cv2.resize(img,(img_width, img_height)) # Resizing all the images\r\n",
        "    image=[]\r\n",
        "    image.append(img)\r\n",
        "    image_test = np.array(image)\r\n",
        "    test_image_flatten = image_test.reshape(image_test.shape[0],-1)\r\n",
        "    test_image_normalized = test_image_flatten/255.\r\n",
        "    #print (test_image_normalized.shape)\r\n",
        "    x_ = tf.cast(test_image_normalized, tf.float32)\r\n",
        "    y = tf.nn.softmax(tf.matmul(x_, W) + b)\r\n",
        "    #print (sess.run(y))\r\n",
        "    indx = sess.run(tf.argmax(y,1))\r\n",
        "    if indx==1:\r\n",
        "        print (\"smile\")\r\n",
        "    elif indx==0:\r\n",
        "        print (\"neutral\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1: None\n",
            "neutral\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}